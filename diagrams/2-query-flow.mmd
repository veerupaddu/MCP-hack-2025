sequenceDiagram
    participant User
    participant Modal
    participant QueryEntrypoint as query()
    participant RAGModel
    participant Embeddings as HuggingFaceEmbeddings<br/>(CUDA)
    participant ChromaRetriever as RemoteChromaRetriever
    participant ChromaDB as Remote ChromaDB
    participant LLM as Mistral-7B<br/>(A10G GPU)
    participant RAGChain as LangChain RAG

    User->>Modal: modal run modal-rag.py::query --question "..."
    Modal->>QueryEntrypoint: Execute local entrypoint
    QueryEntrypoint->>RAGModel: Instantiate RAGModel()
    
    Note over RAGModel: @modal.enter() lifecycle
    RAGModel->>Embeddings: Load embedding model (CUDA)
    RAGModel->>ChromaDB: Connect to remote service
    RAGModel->>LLM: Load Mistral-7B (A10G GPU)
    RAGModel->>RAGModel: Initialize RemoteChromaRetriever
    
    QueryEntrypoint->>RAGModel: query.remote(question)
    
    RAGModel->>ChromaRetriever: Create retriever instance
    RAGModel->>RAGChain: Build RAG chain
    
    RAGChain->>ChromaRetriever: Retrieve relevant docs
    ChromaRetriever->>Embeddings: embed_query(question)
    Embeddings-->>ChromaRetriever: Query embedding
    ChromaRetriever->>ChromaDB: query(embedding, k=3)
    ChromaDB-->>ChromaRetriever: Top-k documents
    ChromaRetriever-->>RAGChain: Return documents
    
    RAGChain->>LLM: Generate answer with context
    LLM-->>RAGChain: Generated answer
    RAGChain-->>RAGModel: Return result
    
    RAGModel-->>QueryEntrypoint: Return {answer, sources}
    QueryEntrypoint-->>User: Display answer + sources
